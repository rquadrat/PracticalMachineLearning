---
title: "Fitness Tracker Machine Learning Analysis"
author: "Roland Rodde"
date: "03/22/2015"
output: html_document
---

## Experimental Data
Test set and training set data are taken from <http://groupware.les.inf.puc-rio.br/har>. The test set and training set data is loaded into R with the read.csv() command. Each record from the test and training set data consists of 160 measurements. See the following list for details:

```{r, echo=F}
training<-read.csv('pml-training.csv')
test<-read.csv('pml-testing.csv')

library("caret")

names(test)
```

These measurements were taken during a lift weighting exercise (Unilateral Dumbbell Biceps Curl). The exercise where carried out by 6 different probands in 5 different ways. Way "A" is the correct way of performing the exercise, the ways "B" to "E" correspond to common mistakes. 4 Different sensors were collecting data: One at the belt, one at the arm, one at the forearm and one at the dumbbell.

```{r, echo=F}
summary(training$user_name)
summary(training$classe)
```
The training set consists of 19622 measurements, the test data set consists of 20 measurements.

## Data Selection and Data Normalisation

First all columns with no/small variation are removed. This is done with the nearZeroVar-function from the caret package.
```{r}
rmindex<-nearZeroVar(training)
redtraining<-training[-rmindex]
redtest<-test[-rmindex]
```
After this removal only 99 from the initial 159 are still present. In a next step all columns mainly consisting of NA's are removed. Therefore we find all columns that have less than 95 % NA's. 
```{r}
naindex<-colSums(is.na(redtraining))/dim(redtraining)[1]<0.95
redtraining<-redtraining[,naindex]
redtest<-redtest[,naindex]
```
After this step only 58 data columns are left. Now we skip the columns 1 (just an id), 2 (the proband doing the exercise), 5 (if a new window starts), and 6 (number of the window). No imputation is necessary as there are no missing values in the data columns kept up to now.

```{r}
sum(is.na(redtraining))
sum(is.na(redtest))
```

Next we carry out some preprocessing on the data. First we normalise and center the data to make the impact of different columns comparable. In a second step we carry out a Principal Component Analysis (PCA) to reduce the dimension of the data further. In this step we keep 95 % of the variance in the data. 

```{r, eval=FALSE}
normalize<-preProcess(redtraining[,-55], method=c("center", "scale"))

redtraining<-predict(normalize, redtraining[,-55])
redtest<-predict(normalize, redtest[,-55])

pca<-preProcess(redtraining, method="pca")

redtraining<-predict(pca, redtraining)
redtest<-predict(pca, redtest)
```

## Model Selection

A Feed Forward Neuronal Network from the nnet package was used to train a classifier. K-fold cross validation with k=10 was carried out. 
Three different values for the number of neurons in the hidden layer were applied: 5, 10 and 20 hidden neurons leading to a total number of 165, 325 and 645 
weights. The second parameter for tuning was the decay parameter with values of 0, 0.1 and 0.2. The maximum number of iterations was set to 250 as 100 seemed to be too low.

```{r, eval=FALSE}
model<-train(redtraining, ytraining, method = "nnet", maxit=250, tuneGrid = data.frame(size=c(5,10,20), decay=c(0,0.1,0.2)),
              trControl = trainControl(method = "cv", number = 10))
```

The following commands show some key properties of the learned neuronal net:

```{r, eval=FALSE}
model$results

  size decay  Accuracy     Kappa  AccuracySD     KappaSD
1    5   0.0 0.6317438 0.5338787 0.025576024 0.032332458
2   10   0.1 0.7603713 0.6963699 0.016349374 0.020778318
3   20   0.2 0.8709094 0.8366287 0.006152087 0.007797355

model$resample

    Accuracy     Kappa Resample
1  0.8654434 0.8297783   Fold01
2  0.8792049 0.8469765   Fold02
3  0.8647959 0.8286268   Fold05
4  0.8751910 0.8421985   Fold04
5  0.8756371 0.8424655   Fold03
6  0.8705403 0.8361993   Fold06
7  0.8665308 0.8310656   Fold09
8  0.8669725 0.8317041   Fold08
9  0.8802853 0.8486515   Fold07
10 0.8644931 0.8286206   Fold10

model$finalModel

a 26-20-5 network with 645 weights
inputs: PC1 PC2 PC3 PC4 PC5 PC6 PC7 PC8 PC9 PC10 PC11 PC12 PC13 PC14 PC15 PC16 PC17 PC18 PC19 PC20 PC21 PC22 PC23 PC24 PC25 PC26 
output(s): .outcome 
options were - softmax modelling  decay=0.2
```
 
## Model Peformance

The model was evaluated on the training set and on the test set. First the predicted classes for the test and training set were calculated using the predict-function. For the training data the real labels were given and so a confusion matrix was calculated.

```{r, eval=FALSE}
predtraining<-predict(model, redtraining)
predtest<-predict(model, redtest)

confusionMatrix(predtraining, ytraining)

          Reference
Prediction    A    B    C    D    E
         A 5379  186   26   29   19
         B  104 3243  155   76  108
         C   43  217 2958  320  143
         D   36   58  199 2745   78
         E   18   93   84   46 3259

Overall Statistics
                                          
               Accuracy : 0.8961          
                 95% CI : (0.8918, 0.9004)
    No Information Rate : 0.2844          
    P-Value [Acc > NIR] : < 2.2e-16       
                                          
                  Kappa : 0.8686          
 Mcnemars Test P-Value : 9.292e-16       

Statistics by Class:

                     Class: A Class: B Class: C Class: D Class: E
Sensitivity            0.9640   0.8541   0.8644   0.8535   0.9035
Specificity            0.9815   0.9720   0.9554   0.9774   0.9850
Pos Pred Value         0.9539   0.8798   0.8036   0.8809   0.9311
Neg Pred Value         0.9856   0.9652   0.9709   0.9715   0.9784
Prevalence             0.2844   0.1935   0.1744   0.1639   0.1838
Detection Rate         0.2741   0.1653   0.1507   0.1399   0.1661
Detection Prevalence   0.2874   0.1879   0.1876   0.1588   0.1784
Balanced Accuracy      0.9727   0.9131   0.9099   0.9155   0.9442

```
_________________________

The Analysis can be found at <http://rquadrat.github.io/PracticalMachineLearning/Analysis.html>